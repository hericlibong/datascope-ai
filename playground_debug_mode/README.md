# üéÆ DataScope AI Debug Playground

Un espace de test d√©di√© pour d√©boguer et analyser le pipeline d'IA DataScope avec des logs d√©taill√©s et du tracing avanc√©.

## üìã Description

Le playground_debug_mode fournit :

- **Logging d√©taill√©** : Tra√ßage complet des √©tapes du pipeline
- **Tests par √©tapes** : Ex√©cution step-by-step pour identifier les probl√®mes
- **M√©triques de performance** : Temps d'ex√©cution pour chaque √©tape
- **Sc√©narios de test** : Collection d'articles de test pr√©d√©finis
- **Configuration flexible** : Param√®tres de debug ajustables

## üöÄ Utilisation rapide

### Configuration de base
```bash
# Depuis la racine du projet
cd playground_debug_mode

# V√©rifier la configuration
python run_debug.py --scenario config
```

### Tests simples
```bash
# Test de base avec un article court
python run_debug.py --scenario basic --article short_politics

# Test √©tape par √©tape
python run_debug.py --scenario step-by-step --article tech_article

# Arr√™ter apr√®s l'extraction
python run_debug.py --scenario step-by-step --article tech_article --stop-after extraction
```

### Tests complets
```bash
# Ex√©cuter tous les sc√©narios de test
python run_debug.py --scenario all
```

## üìÅ Structure

```
playground_debug_mode/
‚îú‚îÄ‚îÄ __init__.py              # Module principal
‚îú‚îÄ‚îÄ debug_utils.py           # Utilitaires de logging et debug
‚îú‚îÄ‚îÄ debug_pipeline.py        # Wrapper debug pour le pipeline
‚îú‚îÄ‚îÄ test_scenarios.py        # Sc√©narios de test pr√©d√©finis
‚îú‚îÄ‚îÄ config.py               # Configuration du debug
‚îú‚îÄ‚îÄ run_debug.py            # Script principal d'ex√©cution
‚îú‚îÄ‚îÄ README.md               # Cette documentation
‚îú‚îÄ‚îÄ logs/                   # Dossier des logs (auto-cr√©√©)
‚îî‚îÄ‚îÄ output/                 # Dossier des r√©sultats (auto-cr√©√©)
```

## üîß Configuration

### Variables d'environnement requises
- `OPENAI_API_KEY` : Cl√© API OpenAI (obligatoire)

### Variables d'environnement optionnelles
- `OPENAI_MODEL` : Mod√®le OpenAI √† utiliser
- `DEBUG` : Mode debug Django
- `SECRET_KEY` : Cl√© secr√®te Django

### Param√®tres de debug

Les param√®tres peuvent √™tre modifi√©s dans `config.py` :

```python
# Logging
LOG_LEVEL = "DEBUG"
ENABLE_DETAILED_LOGGING = True
TRUNCATE_LOGS_AT = 500

# Performance
ENABLE_STEP_TIMING = True
SAVE_INTERMEDIATE_RESULTS = True

# Tests
TEST_TIMEOUT = 60
```

## üìä Types de logs

### üîß Logs d'√©tapes
```
2024-01-20 10:30:15 - debug_pipeline - INFO - üîß STEP: PIPELINE START | Data: {...}
```

### ‚è±Ô∏è Logs de performance
```
2024-01-20 10:30:20 - debug_pipeline - INFO - ‚è±Ô∏è TIMING: extraction took 2.34s
```

### ‚úÖ Logs de r√©sultats
```
2024-01-20 10:30:25 - debug_pipeline - INFO - ‚úÖ RESULT: scoring -> 7.5
```

### ‚ùå Logs d'erreurs
```
2024-01-20 10:30:30 - debug_pipeline - ERROR - ‚ùå ERROR in extraction: API timeout
```

## üß™ Sc√©narios de test pr√©d√©finis

### Articles disponibles
- `short_politics` : Article politique court
- `tech_article` : Article sur la technologie
- `climate_article` : Article sur le climat
- `economic_article` : Article √©conomique

### Types de tests
- **basic** : Test complet du pipeline
- **step-by-step** : Ex√©cution √©tape par √©tape
- **all** : Tous les sc√©narios avec tous les articles

## üìã √âtapes disponibles pour step-by-step

1. **validation** : Validation de la longueur du texte
2. **extraction** : Extraction d'entit√©s (personnes, organisations)
3. **scoring** : Calcul du score de qualit√©
4. **angles** : G√©n√©ration des angles √©ditoriaux
5. **keywords** : Extraction des mots-cl√©s par angle

## üîç Exemples d'utilisation

### Test de debug complet
```python
from playground_debug_mode import DebugPipeline

# Cr√©er le pipeline de debug
debug_pipeline = DebugPipeline()

# Analyser un article avec logs d√©taill√©s
article = "Votre article de test ici..."
result = debug_pipeline.run(article, user_id="test_user")
```

### Test √©tape par √©tape
```python
# Ex√©cuter seulement jusqu'√† l'extraction
results = debug_pipeline.debug_step_by_step(
    article, 
    stop_after="extraction"
)
print(f"Extraction result: {results['extraction']}")
```

### Logging personnalis√©
```python
from playground_debug_mode.debug_utils import setup_debug_logging

# Cr√©er un logger personnalis√©
logger = setup_debug_logging("/path/to/custom.log")
logger.log_step("Custom step", {"data": "example"})
```

## üõ†Ô∏è D√©pannage

### Erreur "No module named ai_engine"
Assurez-vous d'ex√©cuter le script depuis la racine du projet ou que le PYTHONPATH inclut la racine.

### Erreur API OpenAI
V√©rifiez que `OPENAI_API_KEY` est d√©finie dans vos variables d'environnement.

### Erreur Django SECRET_KEY
D√©finissez `SECRET_KEY` dans vos variables d'environnement ou utilisez une valeur par d√©faut pour les tests.

## üìà M√©triques et monitoring

Le playground collecte automatiquement :
- Temps d'ex√©cution pour chaque √©tape
- Taille des donn√©es √† chaque √©tape
- Erreurs et exceptions
- R√©sultats interm√©diaires

Les logs sont sauvegard√©s dans `logs/` avec timestamp pour analyse ult√©rieure.

## üîó Int√©gration avec le pipeline principal

Le playground utilise le pipeline principal (`ai_engine.pipeline`) sans modification, ajoutant seulement une couche de debug et de logging.

## üìù Labels GitHub

Ce module correspond √† l'issue avec les labels :
- `debug` : Fonctionnalit√©s de d√©bogage
- `devtools` : Outils de d√©veloppement  
- `test` : Infrastructure de test