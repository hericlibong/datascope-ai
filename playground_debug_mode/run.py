#!/usr/bin/env python3
"""
Runner principal pour le playground debug mode
Permet de lancer facilement les diff√©rents tests avec des options
"""

import sys
import os
from pathlib import Path
import argparse

# Ajouter le r√©pertoire parent au PYTHONPATH pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from playground_debug_mode.logger import debug_logger


def run_extraction_tests():
    """Lance les tests d'extraction"""
    debug_logger.info("üöÄ Lancement des tests d'extraction...")
    try:
        from playground_debug_mode.test_extraction import main as extraction_main
        extraction_main()
        return True
    except Exception as e:
        debug_logger.error("Erreur lors des tests d'extraction", error=e)
        return False


def run_angles_tests():
    """Lance les tests de g√©n√©ration d'angles"""
    debug_logger.info("üöÄ Lancement des tests d'angles...")
    try:
        from playground_debug_mode.test_angles import main as angles_main
        angles_main()
        return True
    except Exception as e:
        debug_logger.error("Erreur lors des tests d'angles", error=e)
        return False


def run_connectors_tests():
    """Lance les tests de connecteurs"""
    debug_logger.info("üöÄ Lancement des tests de connecteurs...")
    try:
        from playground_debug_mode.test_connectors import main as connectors_main
        connectors_main()
        return True
    except Exception as e:
        debug_logger.error("Erreur lors des tests de connecteurs", error=e)
        return False


def run_pipeline_tests():
    """Lance les tests du pipeline complet"""
    debug_logger.info("üöÄ Lancement des tests de pipeline...")
    try:
        from playground_debug_mode.test_pipeline import main as pipeline_main
        pipeline_main()
        return True
    except Exception as e:
        debug_logger.error("Erreur lors des tests de pipeline", error=e)
        return False


def run_demo():
    """Lance la d√©monstration sans r√©seau"""
    debug_logger.info("üöÄ Lancement de la d√©monstration...")
    try:
        from playground_debug_mode.demo import main as demo_main
        demo_main()
        return True
    except Exception as e:
        debug_logger.error("Erreur lors de la d√©monstration", error=e)
        return False


def run_all_tests():
    """Lance tous les tests dans l'ordre"""
    debug_logger.separator("üß™ Ex√©cution compl√®te - Playground Debug Mode", "=", 100)
    debug_logger.info("D√©marrage de tous les tests du playground")
    
    tests = [
        ("Extraction", run_extraction_tests),
        ("Angles", run_angles_tests),
        ("Connecteurs", run_connectors_tests),
        ("Pipeline", run_pipeline_tests),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        debug_logger.separator(f"Test {test_name}", "-", 60)
        success = test_func()
        results[test_name] = success
        
        if success:
            debug_logger.success(f"‚úÖ Test {test_name} termin√© avec succ√®s")
        else:
            debug_logger.error(f"‚ùå Test {test_name} √©chou√©")
    
    # R√©sum√© final
    debug_logger.separator("R√©sum√© des tests", "=", 100)
    
    successful_tests = sum(1 for success in results.values() if success)
    total_tests = len(results)
    
    debug_logger.metrics("R√©sultats finaux",
                        tests_reussis=successful_tests,
                        tests_totaux=total_tests,
                        taux_reussite=f"{(successful_tests/total_tests*100):.1f}%")
    
    for test_name, success in results.items():
        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHOU√â"
        debug_logger.info(f"{test_name}: {status}")
    
    if successful_tests == total_tests:
        debug_logger.success("üéâ Tous les tests sont r√©ussis!")
    else:
        debug_logger.warning(f"‚ö†Ô∏è {total_tests - successful_tests} test(s) ont √©chou√©")
    
    debug_logger.info("üìÅ Consultez le dossier 'outputs' pour les d√©tails des r√©sultats")


def check_environment():
    """V√©rifie que l'environnement est correctement configur√©"""
    debug_logger.separator("V√©rification de l'environnement", "-", 50)
    
    # V√©rifier la cl√© API OpenAI
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        debug_logger.success("Cl√© API OpenAI configur√©e", key_length=len(openai_key))
    else:
        debug_logger.warning("‚ö†Ô∏è Cl√© API OpenAI non trouv√©e")
        debug_logger.info("Pour utiliser l'IA, d√©finissez OPENAI_API_KEY dans votre environnement")
        debug_logger.info("export OPENAI_API_KEY='votre_cl√©_ici'")
    
    # V√©rifier les d√©pendances importantes
    try:
        import ai_engine
        debug_logger.success("Module ai_engine disponible")
    except ImportError as e:
        debug_logger.error("Module ai_engine non disponible", error=e)
    
    try:
        from langchain import __version__ as langchain_version
        debug_logger.success("LangChain disponible", version=langchain_version)
    except ImportError as e:
        debug_logger.error("LangChain non disponible", error=e)
    
    # V√©rifier le dossier de sortie
    outputs_dir = Path(__file__).parent / "outputs"
    if outputs_dir.exists():
        debug_logger.success("Dossier outputs disponible", path=str(outputs_dir))
    else:
        debug_logger.warning("Dossier outputs manquant, tentative de cr√©ation...")
        try:
            outputs_dir.mkdir(exist_ok=True)
            debug_logger.success("Dossier outputs cr√©√©")
        except Exception as e:
            debug_logger.error("Impossible de cr√©er le dossier outputs", error=e)


def main():
    """Fonction principale avec gestion des arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description="Playground Debug Mode - DataScope AI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python run.py --all              # Lance tous les tests
  python run.py --extraction       # Tests d'extraction seulement
  python run.py --angles           # Tests d'angles seulement
  python run.py --connectors       # Tests de connecteurs seulement
  python run.py --pipeline         # Tests de pipeline seulement
  python run.py --check            # V√©rification de l'environnement
  python run.py --demo             # D√©monstration sans r√©seau
        """
    )
    
    parser.add_argument("--all", action="store_true", 
                       help="Lance tous les tests")
    parser.add_argument("--extraction", action="store_true",
                       help="Lance les tests d'extraction")
    parser.add_argument("--angles", action="store_true",
                       help="Lance les tests de g√©n√©ration d'angles")
    parser.add_argument("--connectors", action="store_true",
                       help="Lance les tests de connecteurs")
    parser.add_argument("--pipeline", action="store_true",
                       help="Lance les tests du pipeline complet")
    parser.add_argument("--check", action="store_true",
                       help="V√©rifie l'environnement de d√©veloppement")
    parser.add_argument("--demo", action="store_true",
                       help="Lance une d√©monstration sans r√©seau")
    
    args = parser.parse_args()
    
    # Si aucun argument, afficher l'aide
    if not any(vars(args).values()):
        parser.print_help()
        return
    
    # V√©rification de l'environnement si demand√©e
    if args.check:
        check_environment()
        return
    
    # D√©monstration si demand√©e
    if args.demo:
        run_demo()
        return
    
    # Lancement des tests selon les arguments
    if args.all:
        run_all_tests()
    else:
        if args.extraction:
            run_extraction_tests()
        
        if args.angles:
            run_angles_tests()
        
        if args.connectors:
            run_connectors_tests()
        
        if args.pipeline:
            run_pipeline_tests()


if __name__ == "__main__":
    main()